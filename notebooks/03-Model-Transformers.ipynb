{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Charger les données\n",
    "def load_data(input_file, output_file):\n",
    "    input_sequences = pd.read_csv(input_file)\n",
    "    output_sequences = pd.read_csv(output_file)\n",
    "    return input_sequences, output_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Préparer les données pour Transformers\n",
    "def prepare_transformer_data(input_sequences, output_sequences, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    video_object_ids = input_sequences.iloc[:, :2].values  # `video_id`, `object_id`\n",
    "\n",
    "    # Extraire les séquences d'entrée et de sortie\n",
    "    X = input_sequences.iloc[:, 2:].values  # Frames pour l'encodage\n",
    "    y = output_sequences.iloc[:, 2:].values  # Frames pour le décodage\n",
    "\n",
    "    # Diviser les données en ensembles d'entraînement, validation et test\n",
    "    X_train_val, X_test, y_train_val, y_test, ids_train_val, ids_test = train_test_split(\n",
    "        X, y, video_object_ids, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val, ids_train, ids_val = train_test_split(\n",
    "        X_train_val, y_train_val, ids_train_val, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, X, y, ids):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Séquences d'entrée\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # Séquences de sortie\n",
    "        self.ids = torch.tensor(ids, dtype=torch.long)  # Identifiants (video_id, object_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': self.X[idx],\n",
    "            'target': self.y[idx],\n",
    "            'meta': self.ids[idx]  # Utilisation de 'meta' pour les identifiants\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ajouter un encodage positionnel\n",
    "def add_positional_encoding(data, sequence_length, d_model):\n",
    "    position = torch.arange(sequence_length).unsqueeze(1).float()\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pos_encoding = torch.zeros(sequence_length, d_model)\n",
    "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "    return data + pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensors(X_train, X_val, X_test, y_train, y_val, y_test, device='cpu'):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    \n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "    \n",
    "    return X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train, y_train, ids_train, X_val, y_val, ids_val, X_test, y_test, ids_test, batch_size=32):\n",
    "    train_dataset = VideoDataset(X_train, y_train, ids_train)\n",
    "    val_dataset = VideoDataset(X_val, y_val, ids_val)\n",
    "    test_dataset = VideoDataset(X_test, y_test, ids_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chargement des données\n",
    "input_file = 'C:/Users/h/Desktop/MASTER IAAD/S3/projet2/Data/annotations_transformers/input_sequences.csv'\n",
    "output_file = 'C:/Users/h/Desktop/MASTER IAAD/S3/projet2/Data/annotations_transformers/output_sequences.csv'\n",
    "\n",
    "input_sequences, output_sequences = load_data(input_file, output_file)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test = prepare_transformer_data(\n",
    "    input_sequences, output_sequences\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor = convert_to_tensors(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h\\AppData\\Local\\Temp\\ipykernel_18164\\714682715.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)  # Séquences d'entrée\n",
      "C:\\Users\\h\\AppData\\Local\\Temp\\ipykernel_18164\\714682715.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y, dtype=torch.float32)  # Séquences de sortie\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    X_train_tensor, y_train_tensor, ids_train,\n",
    "    X_val_tensor, y_val_tensor, ids_val,\n",
    "    X_test_tensor, y_test_tensor, ids_test,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de calcul de l'ADE et du FDE\n",
    "def compute_ade_fde(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calcule ADE (Average Displacement Error) et FDE (Final Displacement Error).\n",
    "    \"\"\"\n",
    "    ade = torch.mean(torch.sqrt(torch.sum((predictions - targets) ** 2, dim=-1)))\n",
    "    fde = torch.sqrt(torch.sum((predictions[:, -1] - targets[:, -1]) ** 2, dim=-1)).mean()\n",
    "    return ade.item(), fde.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Transformer amélioré\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=128, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings pour les entrées et les sorties\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.output_embedding = nn.Linear(output_dim, d_model)\n",
    "        \n",
    "        # Encodeur LSTM pour capturer les dépendances temporelles\n",
    "        self.lstm_encoder = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        \n",
    "        # Encodeur Transformer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout\n",
    "            ), \n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Décodeur Transformer avec attention multi-têtes\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout\n",
    "            ), \n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "        \n",
    "        # Tête de sortie\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Embedding des entrées et des sorties\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.output_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Ajouter l'encodage positionnel\n",
    "        src = add_positional_encoding(src, src.size(0), self.d_model)\n",
    "        tgt = add_positional_encoding(tgt, tgt.size(0), self.d_model)\n",
    "        \n",
    "        # Passage à travers l'encodeur LSTM\n",
    "        lstm_out, _ = self.lstm_encoder(src)\n",
    "        \n",
    "        # Passage à travers l'encodeur Transformer\n",
    "        memory = self.transformer_encoder(lstm_out)\n",
    "        \n",
    "        # Passage à travers le décodeur Transformer\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        \n",
    "        # Sortie du modèle\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialisation du modèle\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = y_train_tensor.shape[1]\n",
    "model = TransformerModel(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de validation\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    ade, fde = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, targets)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, output_dim), targets.view(-1, output_dim))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            batch_ade, batch_fde = compute_ade_fde(outputs, targets)\n",
    "            ade += batch_ade\n",
    "            fde += batch_fde\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    ade /= len(val_loader)\n",
    "    fde /= len(val_loader)\n",
    "    \n",
    "    return val_loss, ade, fde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        ade, fde = 0.0, 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs, targets)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, output_dim), targets.view(-1, output_dim))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_ade, batch_fde = compute_ade_fde(outputs, targets)\n",
    "            ade += batch_ade\n",
    "            fde += batch_fde\n",
    "        \n",
    "        running_loss /= len(train_loader)\n",
    "        ade /= len(train_loader)\n",
    "        fde /= len(train_loader)\n",
    "        \n",
    "        val_loss, val_ade, val_fde = validate(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss:.4f}, Train ADE: {ade:.4f}, Train FDE: {fde:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation ADE: {val_ade:.4f}, Validation FDE: {val_fde:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0422, Train ADE: 0.7153, Train FDE: 0.3268\n",
      "Validation Loss: 0.0345, Validation ADE: 0.6494, Validation FDE: 0.0154\n",
      "Epoch 2/10, Train Loss: 0.0343, Train ADE: 0.6499, Train FDE: 0.1061\n",
      "Validation Loss: 0.0353, Validation ADE: 0.6650, Validation FDE: 0.0593\n",
      "Epoch 3/10, Train Loss: 0.0050, Train ADE: 0.1957, Train FDE: 0.1025\n",
      "Validation Loss: 0.0008, Validation ADE: 0.0846, Validation FDE: 0.0187\n",
      "Epoch 4/10, Train Loss: 0.0006, Train ADE: 0.0794, Train FDE: 0.0491\n",
      "Validation Loss: 0.0003, Validation ADE: 0.0513, Validation FDE: 0.0211\n",
      "Epoch 5/10, Train Loss: 0.0003, Train ADE: 0.0574, Train FDE: 0.0307\n",
      "Validation Loss: 0.0002, Validation ADE: 0.0435, Validation FDE: 0.0181\n",
      "Epoch 6/10, Train Loss: 0.0002, Train ADE: 0.0492, Train FDE: 0.0216\n",
      "Validation Loss: 0.0002, Validation ADE: 0.0453, Validation FDE: 0.0092\n",
      "Epoch 7/10, Train Loss: 0.0002, Train ADE: 0.0423, Train FDE: 0.0143\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0321, Validation FDE: 0.0076\n",
      "Epoch 8/10, Train Loss: 0.0002, Train ADE: 0.0375, Train FDE: 0.0080\n",
      "Validation Loss: 0.0002, Validation ADE: 0.0401, Validation FDE: 0.0039\n",
      "Epoch 9/10, Train Loss: 0.0001, Train ADE: 0.0350, Train FDE: 0.0065\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0264, Validation FDE: 0.0030\n",
      "Epoch 10/10, Train Loss: 0.0001, Train ADE: 0.0325, Train FDE: 0.0050\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0302, Validation FDE: 0.0027\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "import torch\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save({ \n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  \n",
    "}, 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Recharger l'état du modèle et de l'optimiseur de manière sécurisée\u001b[39;00m\n\u001b[0;32m      3\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodel_checkpoint.pth\u001b[39m\u001b[39m'\u001b[39m, weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel_checkpoint.pth\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m optimizer\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Users/h/Desktop/MASTER IAAD/S3/projet2/Codes/model_checkpoint.pth\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Recharger l'état du modèle et de l'optimiseur de manière sécurisée\n",
    "checkpoint = torch.load('model_checkpoint.pth', weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_checkpoint.pth'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Optionnel : Si vous avez sauvegardé l'epoch, vous pouvez aussi récupérer cette information\n",
    "# epoch = checkpoint['epoch']\n",
    "model.eval()  # Passe le modèle en mode évaluation si vous voulez faire des prédictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Charger la structure du modèle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(input_dim\u001b[39m=\u001b[39minput_dim, output_dim\u001b[39m=\u001b[39moutput_dim)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Charger les poids sauvegardés\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mC:/Users/h/Desktop/MASTER IAAD/S3/projet2/Codes/model.pth\u001b[39m\u001b[39m'\u001b[39m, map_location\u001b[39m=\u001b[39mdevice))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TransformerModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Charger la structure du modèle\n",
    "model = TransformerModel(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "# Charger les poids sauvegardés\n",
    "model.load_state_dict(torch.load('C:/Users/h/Desktop/MASTER IAAD/S3/projet2/Codes/model.pth', map_location=device))\n",
    "\n",
    "# Déplacer le modèle sur le bon appareil (CPU ou GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Mettre le modèle en mode évaluation\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Charger les données\n",
    "def load_data(input_file, output_file):\n",
    "    input_sequences = pd.read_csv(input_file)\n",
    "    output_sequences = pd.read_csv(output_file)\n",
    "    return input_sequences, output_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Préparer les données pour Transformers\n",
    "def prepare_transformer_data(input_sequences, output_sequences, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    video_object_ids = input_sequences.iloc[:, :2].values  # `video_id`, `object_id`\n",
    "\n",
    "    # Extraire les séquences d'entrée et de sortie\n",
    "    X = input_sequences.iloc[:, 2:].values  # Frames pour l'encodage\n",
    "    y = output_sequences.iloc[:, 2:].values  # Frames pour le décodage\n",
    "\n",
    "    # Diviser les données en ensembles d'entraînement, validation et test\n",
    "    X_train_val, X_test, y_train_val, y_test, ids_train_val, ids_test = train_test_split(\n",
    "        X, y, video_object_ids, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val, ids_train, ids_val = train_test_split(\n",
    "        X_train_val, y_train_val, ids_train_val, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, X, y, ids):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Séquences d'entrée\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # Séquences de sortie\n",
    "        self.ids = torch.tensor(ids, dtype=torch.long)  # Identifiants (video_id, object_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': self.X[idx],\n",
    "            'target': self.y[idx],\n",
    "            'meta': self.ids[idx]  # Utilisation de 'meta' pour les identifiants\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ajouter un encodage positionnel\n",
    "def add_positional_encoding(data, sequence_length, d_model):\n",
    "    position = torch.arange(sequence_length).unsqueeze(1).float()\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pos_encoding = torch.zeros(sequence_length, d_model)\n",
    "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "    return data + pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensors(X_train, X_val, X_test, y_train, y_val, y_test, device='cpu'):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "    \n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32, device=device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "    \n",
    "    return X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train, y_train, ids_train, X_val, y_val, ids_val, X_test, y_test, ids_test, batch_size=32):\n",
    "    train_dataset = VideoDataset(X_train, y_train, ids_train)\n",
    "    val_dataset = VideoDataset(X_val, y_val, ids_val)\n",
    "    test_dataset = VideoDataset(X_test, y_test, ids_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chargement des données\n",
    "input_file = '../data/annotations_transformers/input_sequences.csv'\n",
    "output_file = '../data/annotations_transformers/output_sequences.csv'\n",
    "\n",
    "input_sequences, output_sequences = load_data(input_file, output_file)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, ids_train, ids_val, ids_test = prepare_transformer_data(\n",
    "    input_sequences, output_sequences\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor = convert_to_tensors(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beeclick\\AppData\\Local\\Temp\\ipykernel_15948\\714682715.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)  # Séquences d'entrée\n",
      "C:\\Users\\Beeclick\\AppData\\Local\\Temp\\ipykernel_15948\\714682715.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y, dtype=torch.float32)  # Séquences de sortie\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    X_train_tensor, y_train_tensor, ids_train,\n",
    "    X_val_tensor, y_val_tensor, ids_val,\n",
    "    X_test_tensor, y_test_tensor, ids_test,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de calcul de l'ADE et du FDE\n",
    "def compute_ade_fde(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calcule ADE (Average Displacement Error) et FDE (Final Displacement Error).\n",
    "    \"\"\"\n",
    "    ade = torch.mean(torch.sqrt(torch.sum((predictions - targets) ** 2, dim=-1)))\n",
    "    fde = torch.sqrt(torch.sum((predictions[:, -1] - targets[:, -1]) ** 2, dim=-1)).mean()\n",
    "    return ade.item(), fde.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Transformer amélioré\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=128, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings pour les entrées et les sorties\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.output_embedding = nn.Linear(output_dim, d_model)\n",
    "        \n",
    "        # Encodeur LSTM pour capturer les dépendances temporelles\n",
    "        self.lstm_encoder = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        \n",
    "        # Encodeur Transformer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout\n",
    "            ), \n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        # Décodeur Transformer avec attention multi-têtes\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout\n",
    "            ), \n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "        \n",
    "        # Tête de sortie\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # Embedding des entrées et des sorties\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.output_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Ajouter l'encodage positionnel\n",
    "        src = add_positional_encoding(src, src.size(0), self.d_model)\n",
    "        tgt = add_positional_encoding(tgt, tgt.size(0), self.d_model)\n",
    "        \n",
    "        # Passage à travers l'encodeur LSTM\n",
    "        lstm_out, _ = self.lstm_encoder(src)\n",
    "        \n",
    "        # Passage à travers l'encodeur Transformer\n",
    "        memory = self.transformer_encoder(lstm_out)\n",
    "        \n",
    "        # Passage à travers le décodeur Transformer\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        \n",
    "        # Sortie du modèle\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beeclick\\Documents\\MIAAD\\S3\\Projet2\\Projet\\Transformers-Based-Traffic-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialisation du modèle\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = y_train_tensor.shape[1]\n",
    "model = TransformerModel(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de validation\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    ade, fde = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            outputs = model(inputs, targets)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, output_dim), targets.view(-1, output_dim))\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            batch_ade, batch_fde = compute_ade_fde(outputs, targets)\n",
    "            ade += batch_ade\n",
    "            fde += batch_fde\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    ade /= len(val_loader)\n",
    "    fde /= len(val_loader)\n",
    "    \n",
    "    return val_loss, ade, fde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        ade, fde = 0.0, 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs, targets)\n",
    "            \n",
    "            loss = criterion(outputs.view(-1, output_dim), targets.view(-1, output_dim))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_ade, batch_fde = compute_ade_fde(outputs, targets)\n",
    "            ade += batch_ade\n",
    "            fde += batch_fde\n",
    "        \n",
    "        running_loss /= len(train_loader)\n",
    "        ade /= len(train_loader)\n",
    "        fde /= len(train_loader)\n",
    "        \n",
    "        val_loss, val_ade, val_fde = validate(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss:.4f}, Train ADE: {ade:.4f}, Train FDE: {fde:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation ADE: {val_ade:.4f}, Validation FDE: {val_fde:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0376, Train ADE: 0.6882, Train FDE: 0.3101\n",
      "Validation Loss: 0.0346, Validation ADE: 0.6562, Validation FDE: 0.0016\n",
      "Epoch 2/10, Train Loss: 0.0343, Train ADE: 0.6498, Train FDE: 0.1222\n",
      "Validation Loss: 0.0341, Validation ADE: 0.6478, Validation FDE: 0.0119\n",
      "Epoch 3/10, Train Loss: 0.0055, Train ADE: 0.1990, Train FDE: 0.1126\n",
      "Validation Loss: 0.0004, Validation ADE: 0.0534, Validation FDE: 0.0332\n",
      "Epoch 4/10, Train Loss: 0.0004, Train ADE: 0.0611, Train FDE: 0.0495\n",
      "Validation Loss: 0.0002, Validation ADE: 0.0367, Validation FDE: 0.0260\n",
      "Epoch 5/10, Train Loss: 0.0002, Train ADE: 0.0422, Train FDE: 0.0322\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0271, Validation FDE: 0.0125\n",
      "Epoch 6/10, Train Loss: 0.0001, Train ADE: 0.0330, Train FDE: 0.0210\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0305, Validation FDE: 0.0128\n",
      "Epoch 7/10, Train Loss: 0.0001, Train ADE: 0.0298, Train FDE: 0.0152\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0217, Validation FDE: 0.0098\n",
      "Epoch 8/10, Train Loss: 0.0001, Train ADE: 0.0259, Train FDE: 0.0101\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0268, Validation FDE: 0.0061\n",
      "Epoch 9/10, Train Loss: 0.0001, Train ADE: 0.0239, Train FDE: 0.0065\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0177, Validation FDE: 0.0020\n",
      "Epoch 10/10, Train Loss: 0.0001, Train ADE: 0.0225, Train FDE: 0.0049\n",
      "Validation Loss: 0.0001, Validation ADE: 0.0201, Validation FDE: 0.0014\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "import torch\n",
    "torch.save(model.state_dict(), '../models/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save({ \n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  \n",
    "}, '../models/model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_checkpoint.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Recharger l'état du modèle et de l'optimiseur de manière sécurisée\u001b[39;00m\n\u001b[0;32m      3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/model_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Optionnel : Si vous avez sauvegardé l'epoch, vous pouvez aussi récupérer cette information\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# epoch = checkpoint['epoch']\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_checkpoint.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Recharger l'état du modèle et de l'optimiseur de manière sécurisée\n",
    "checkpoint = torch.load('../models/model_checkpoint.pth', weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_checkpoint.pth'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Optionnel : Si vous avez sauvegardé l'epoch, vous pouvez aussi récupérer cette information\n",
    "# epoch = checkpoint['epoch']\n",
    "model.eval()  # Passe le modèle en mode évaluation si vous voulez faire des prédictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beeclick\\Documents\\MIAAD\\S3\\Projet2\\Projet\\Transformers-Based-Traffic-Prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Beeclick\\AppData\\Local\\Temp\\ipykernel_15948\\3159395814.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../models/model.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embedding): Linear(in_features=30, out_features=128, bias=True)\n",
       "  (output_embedding): Linear(in_features=15, out_features=128, bias=True)\n",
       "  (lstm_encoder): LSTM(128, 128, batch_first=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Charger la structure du modèle\n",
    "model = TransformerModel(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "# Charger les poids sauvegardés\n",
    "model.load_state_dict(torch.load('../models/model.pth', map_location=device))\n",
    "\n",
    "# Déplacer le modèle sur le bon appareil (CPU ou GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Mettre le modèle en mode évaluation\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
